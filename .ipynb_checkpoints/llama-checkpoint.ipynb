{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fb12af6-3236-4b4f-abe6-1fce190ee4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-3.16.4-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.7.4.3 in ./.local/lib/python3.8/site-packages (from pypdf) (4.8.0)\n",
      "Downloading pypdf-3.16.4-py3-none-any.whl (276 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.6/276.6 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdf\n",
      "Successfully installed pypdf-3.16.4\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU \\\n",
    "  pypdf \\\n",
    "  transformers==4.31.0 \\\n",
    "  sentence-transformers==2.2.2 \\\n",
    "  pinecone-client==2.2.2 \\\n",
    "  datasets==2.14.0 \\\n",
    "  accelerate==0.21.0 \\\n",
    "  einops==0.6.1 \\\n",
    "  langchain==0.0.303 \\\n",
    "  xformers==0.0.20 \\\n",
    "  bitsandbytes==0.41.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7b8c600-4086-4aeb-9b0d-fadb7054c3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "URL = 'http://genai.spcinfra.cloud/llm/v1/completions'\n",
    "MAX_TOKENS = 1000\n",
    "TEMPERATURE = 1.0\n",
    "\n",
    "# milvus \n",
    "ENTITIES = 3000\n",
    "DIMENSION = 384\n",
    "HOST = \"localhost\"\n",
    "PORT = \"19530\"\n",
    "COLLECTION = \"collection_1\"\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a500ce14-0af4-4cc4-9734-eeb9a9daa9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9a8741cac94ca0a339d9c599ae9a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d902d38bda456fa145aa86012e17ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1d5835c7d646798d5a36f8841a33ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'pubDate', 'guid', 'link', 'description'],\n",
       "        num_rows: 6168\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# data_set = load_dataset(\"csv\", data_files=\"./data/news_summary.csv\")\n",
    "data_set = load_dataset(\"csv\", data_files=\"./data/bbc_news.csv\")\n",
    "data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fbc33a90-e58d-482f-9fa7-4371cd716bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start connecting to Milvus\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import (\n",
    "    connections,\n",
    "    utility,\n",
    "    FieldSchema, CollectionSchema, DataType,\n",
    "    Collection,\n",
    ")\n",
    "\n",
    "print(\"start connecting to Milvus\")\n",
    "connections.connect(\"default\", host=HOST, port=PORT)\n",
    "\n",
    "if utility.has_collection(COLLECTION):\n",
    "    utility.drop_collection(COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "63bb6223-ad1e-493b-b8f9-95e28d8bef01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create collection collection_1\n"
     ]
    }
   ],
   "source": [
    "fields = [\n",
    "    FieldSchema(name=\"articleID\", dtype=DataType.INT64, is_primary=True, auto_id=False),\n",
    "    FieldSchema(name=\"title\", dtype=DataType.VARCHAR, max_length=1000),\n",
    "    FieldSchema(name=\"description\", dtype=DataType.VARCHAR, max_length=30000),\n",
    "    FieldSchema(name=\"embeddings\", dtype=DataType.FLOAT_VECTOR, dim=DIMENSION)\n",
    "]\n",
    "\n",
    "schema = CollectionSchema(fields, \"my schema\")\n",
    "\n",
    "print(\"Create collection %s\" %COLLECTION)\n",
    "collection = Collection(COLLECTION, schema, consistency_level=\"Strong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "48c92844-c6a3-4bf4-8295-30221fff73e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5fddc0e32b429e81d023cc3c7c36fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start inserting entities\n",
      "Number of entities in Milvus: 6168\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "embeddings = model.encode(data_set[\"train\"][\"title\"], show_progress_bar=True, normalize_embeddings=True)\n",
    "\n",
    "entities = [\n",
    "    [i for i in range(6168)],\n",
    "    data_set[\"train\"][\"title\"],\n",
    "    data_set[\"train\"][\"description\"],\n",
    "    [x for x in embeddings]\n",
    "    ]\n",
    "\n",
    "\n",
    "print(\"Start inserting entities\")\n",
    "collection.insert(entities)\n",
    "collection.flush()\n",
    "\n",
    "print(f\"Number of entities in Milvus: {collection.num_entities}\")  # check the num_entites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8c1fe981-eb7e-40e5-87ca-9948b3d940d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Creating index IVF_FLAT\n",
      "Start loading\n"
     ]
    }
   ],
   "source": [
    "print(\"Start Creating index IVF_FLAT\")\n",
    "index = {\n",
    "    \"index_type\": \"IVF_FLAT\",\n",
    "    \"metric_type\": \"L2\",\n",
    "    \"params\": {\"nlist\": 128},\n",
    "}\n",
    "\n",
    "collection.create_index(\"embeddings\", index)\n",
    "print(\"Start loading\")\n",
    "collection.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0532d30e-32ab-4558-88c9-47d297d82087",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"who won world snooker championship in 2022\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5a156dc1-38ea-4c6a-a66c-b3e34bc4b593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start searching based on vector similarity\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739a1c8f3e6a4efa81084427fbf4daae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit: id: 2419, distance: 0.4780420660972595, entity: {'title': \"World Snooker Championship 2022: Ronnie O'Sullivan claims record-equalling seventh world title\"}, \n",
      "random field: World Snooker Championship 2022: Ronnie O'Sullivan claims record-equalling seventh world title \n",
      "\n",
      "hit: id: 2204, distance: 0.4799487590789795, entity: {'title': \"World Snooker Championship 2022: Ronnie O'Sullivan extends quarter-final advantage\"}, \n",
      "random field: World Snooker Championship 2022: Ronnie O'Sullivan extends quarter-final advantage \n",
      "\n",
      "hit: id: 2052, distance: 0.4818710684776306, entity: {'title': \"World Snooker Championship 2022: Mark Williams through, Ronnie O'Sullivan leads\"}, \n",
      "random field: World Snooker Championship 2022: Mark Williams through, Ronnie O'Sullivan leads \n",
      "\n",
      "hit: id: 2364, distance: 0.5009726881980896, entity: {'title': \"World Snooker Championship 2022: Ronnie O'Sullivan books final place against Judd Trump\"}, \n",
      "random field: World Snooker Championship 2022: Ronnie O'Sullivan books final place against Judd Trump \n",
      "\n",
      "hit: id: 1975, distance: 0.5011944770812988, entity: {'title': 'World Snooker Championship 2022: John Higgins goes through, Ding Junhui out'}, \n",
      "random field: World Snooker Championship 2022: John Higgins goes through, Ding Junhui out \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Start searching based on vector similarity\")\n",
    "\n",
    "search_params = {\n",
    "    \"metric_type\": \"L2\",\n",
    "    \"params\": {\"nprobe\": 10},\n",
    "}\n",
    "\n",
    "embeddings = model.encode(QUESTION, show_progress_bar=True, normalize_embeddings=True)\n",
    "result = collection.search([embeddings], \"embeddings\", search_params, limit=5, output_fields=[\"title\"])\n",
    "\n",
    "my_content = []\n",
    "for hits in result:\n",
    "    for hit in hits:\n",
    "        my_content.append(hit.entity.get('title'))\n",
    "        print(f\"hit: {hit}, \\nrandom field: {hit.entity.get('title')} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01c8907-9a96-4314-b33e-2e4f37691e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b771bb9e-948d-4ed4-9f6b-de4ca7e46ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Use the following pieces of context to answer the question at the end.\\nIf you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\nUse three sentences maximum and keep the answer as concise as possible.\\nAlways say \"thanks for asking!\" at the end of the answer.\\n[\"World Snooker Championship 2022: Ronnie O\\'Sullivan claims record-equalling seventh world title\", \"World Snooker Championship 2022: Ronnie O\\'Sullivan extends quarter-final advantage\", \"World Snooker Championship 2022: Mark Williams through, Ronnie O\\'Sullivan leads\", \"World Snooker Championship 2022: Ronnie O\\'Sullivan books final place against Judd Trump\", \\'World Snooker Championship 2022: John Higgins goes through, Ding Junhui out\\']\\nQuestion: who won world snooker championship in 2022\\nHelpful Answer:'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    ")\n",
    "prompt = prompt_template.format(context=my_content, question=QUESTION)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3e570ee7-188c-4a42-9ec1-b61f6dce8248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "import requests\n",
    "import json\n",
    "class CustomLLM(LLM):\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        data = {\n",
    "            'model': '/models/llama2/hf/13b-chat',\n",
    "            'prompt': prompt,\n",
    "            'max_tokens': MAX_TOKENS,\n",
    "            'temperature': TEMPERATURE\n",
    "        }\n",
    "        response = requests.post(URL, data=json.dumps(data), headers=headers)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            raise ValueError('response status code is bad')\n",
    "\n",
    "        data = response.json()\n",
    "        return data['choices'][0]['text']\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b936d880-1814-4f90-a4fc-a08d89da4127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThanks for asking! Based on the articles you've provided, Ronnie O'Sullivan won the World Snooker Championship in 2022.\""
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = CustomLLM(n=10)\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b3d447-587c-40fa-bf36-b8ab825a8501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ba6ad0-b3a0-41d7-be95-2f6cd2c85529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee4a8f5-dd56-44d5-af83-b36dcdd5da3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5706433-1bbb-42bf-b441-4e92f11690fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7c4e44-773d-4f68-a9fa-f031a20758f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e733fe54-ed9a-4fa1-aa24-2f2cf91be284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You: Greetings! Mansa Musa was the king of the Mali Empire during the 14th century, and he is widely regarded as one of the wealthiest individuals in African history. Mansa Musa's wealth can be attributed to several factors, including:\n",
      "\n",
      "1. Trade: The Mali Empire was strategically located along the trans-Saharan trade route, which allowed for the exchange of goods between Africa, the Middle East, and Europe. The empire's control over this route enabled Mansa Musa to collect taxes and tariffs on goods passing through his territory, thereby accumulating significant wealth.\n",
      "2. Gold and salt mines: The Mali Empire was home to several gold and salt mines, which were major sources of wealth for the empire. Mansa Musa controlled these mines and profited from the mining industry.\n",
      "3. Agriculture: The Mali Empire was known for its productive agricultural lands, which produced a variety of crops such as grains, cotton, and peanuts. Mansa Musa controlled these lands and collected taxes on the agricultural produce, further adding to his wealth.\n",
      "4. Islamic trade networks: Mansa Musa was a devout Muslim and supported the spread of Islam in West Africa. His relationship with Muslim merchants and traders in the Mediterranean and the Middle East enabled him to access new markets and trade networks, which helped him accumulate wealth.\n",
      "5. Political power: As the king of the Mali Empire, Mansa Musa wielded significant political power and influence. He used this power to ensure the stability and prosperity of his empire, which contributed to his wealth.\n",
      "\n",
      "I hope this answer provides a comprehensive overview of how Mansa Musa accumulated his wealth. Do you have any further questions?"
     ]
    }
   ],
   "source": [
    "from langchain.schema import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You're a very knowledgeable historian who provides accurate and eloquent answers to historical questions.\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "runnable = prompt | llm | StrOutputParser()\n",
    "for chunk in runnable.stream({\"question\": \"How did Mansa Musa accumulate his wealth?\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8c0b32f3-c4ba-41a1-8ce7-043545b2d130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from pydantic import Extra\n",
    "\n",
    "from langchain.schema.language_model import BaseLanguageModel\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForChainRun,\n",
    "    CallbackManagerForChainRun,\n",
    ")\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.prompts.base import BasePromptTemplate\n",
    "\n",
    "\n",
    "class MyCustomChain(Chain):\n",
    "    \"\"\"\n",
    "    An example of a custom chain.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt: BasePromptTemplate\n",
    "    \"\"\"Prompt object to use.\"\"\"\n",
    "    llm: BaseLanguageModel\n",
    "    output_key: str = \"text\"  #: :meta private:\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        extra = Extra.forbid\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Will be whatever keys the prompt expects.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return self.prompt.input_variables\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"Will always return text key.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return [self.output_key]\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        # Your custom chain logic goes here\n",
    "        # This is just an example that mimics LLMChain\n",
    "        prompt_value = self.prompt.format_prompt(**inputs)\n",
    "\n",
    "        # Whenever you call a language model, or another chain, you should pass\n",
    "        # a callback manager to it. This allows the inner run to be tracked by\n",
    "        # any callbacks that are registered on the outer run.\n",
    "        # You can always obtain a callback manager for this by calling\n",
    "        # `run_manager.get_child()` as shown below.\n",
    "        response = self.llm.generate_prompt(\n",
    "            [prompt_value], callbacks=run_manager.get_child() if run_manager else None\n",
    "        )\n",
    "\n",
    "        # If you want to log something about this run, you can do so by calling\n",
    "        # methods on the `run_manager`, as shown below. This will trigger any\n",
    "        # callbacks that are registered for that event.\n",
    "        if run_manager:\n",
    "            run_manager.on_text(\"Log something about this run\")\n",
    "\n",
    "        return {self.output_key: response.generations[0][0].text}\n",
    "\n",
    "    async def _acall(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        # Your custom chain logic goes here\n",
    "        # This is just an example that mimics LLMChain\n",
    "        prompt_value = self.prompt.format_prompt(**inputs)\n",
    "\n",
    "        # Whenever you call a language model, or another chain, you should pass\n",
    "        # a callback manager to it. This allows the inner run to be tracked by\n",
    "        # any callbacks that are registered on the outer run.\n",
    "        # You can always obtain a callback manager for this by calling\n",
    "        # `run_manager.get_child()` as shown below.\n",
    "        response = await self.llm.agenerate_prompt(\n",
    "            [prompt_value], callbacks=run_manager.get_child() if run_manager else None\n",
    "        )\n",
    "\n",
    "        # If you want to log something about this run, you can do so by calling\n",
    "        # methods on the `run_manager`, as shown below. This will trigger any\n",
    "        # callbacks that are registered for that event.\n",
    "        if run_manager:\n",
    "            await run_manager.on_text(\"Log something about this run\")\n",
    "\n",
    "        return {self.output_key: response.generations[0][0].text}\n",
    "\n",
    "    @property\n",
    "    def _chain_type(self) -> str:\n",
    "        return \"my_custom_chain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4b61e4-a783-4513-a548-afbe8c63e48a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50f62e7-bbe4-4cdb-b85a-78df75ea34d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9780ef92-1c71-4b69-864b-347393061364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b37ee-9a1f-4547-85ff-496be5a94632",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c24956-d9b3-4f43-98b9-d9dd6dd9198e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61159fb8-f8f8-497c-a22b-b0873930c548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c634b9-e5da-4eb1-b121-a4d8fe1251c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5481aed6-63a1-489f-8042-52b344703098",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
